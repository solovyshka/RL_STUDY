{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TDqfgGd0bL3C"
   },
   "source": [
    "# Машинное обучение с подкреплением. Сбербанк весна 2021.\n",
    "## Семинар 2: Обзор сред. Q-обучение. Апроксимация Q-функции. \n",
    "\n",
    "### 1. Обзор сред\n",
    "\n",
    "* [Classic Control](https://gym.openai.com/envs/#classic_control)\n",
    "* [Box2D](https://gym.openai.com/envs/#box2d)\n",
    "* [Atari](https://gym.openai.com/envs/#atari)\n",
    "* [Gym Retro](https://openai.com/blog/gym-retro/)\n",
    "* [Mujoco](https://gym.openai.com/envs/#mujoco)\n",
    "* [Robotics](https://gym.openai.com/envs/#robotics)\n",
    "* [Universe](https://openai.com/blog/universe/)\n",
    "* [MineRL](https://minerl.io/) \\(использует проект [malmo](https://www.microsoft.com/en-us/research/project/project-malmo/))\n",
    "* [Starcraft II](https://github.com/deepmind/pysc2)\n",
    "* [Biomechanics: Learning to move](https://www.aicrowd.com/challenges/neurips-2019-learning-to-move-walk-around)\n",
    "* [Procgen](https://openai.com/blog/procgen-benchmark/)\n",
    "* [Halitate on Kaggle](https://www.kaggle.com/c/halite) \n",
    "* [Flatland](https://www.aicrowd.com/challenges/neurips-2020-flatland-challenge)\n",
    "* Настольные игры: Chess, GO и т.д. (множество среда на github)\n",
    "* [Learning to Run a Power Network](https://competitions.codalab.org/competitions/20767) \\(ссылка на [NIPS](https://nips.cc/Conferences/2020/CompetitionTrack))\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajUaHPOJH-3Z"
   },
   "source": [
    "### 2. Q-обучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIX-e9zJKk23"
   },
   "source": [
    "Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение.\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686\">\n",
    "\n",
    "\n",
    "Раскроем скобки:\n",
    "$$Q(s,a)\\leftarrow (1 - \\alpha) \\times Q(s,a)+\\alpha \\times \\big (r(s)+\\gamma\\max_{a'}Q(a',s')\\big ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ79cSTLLPb-"
   },
   "source": [
    "Для обучения будем использовать среду Taxi-v3. Подробнее про данное окружение можно посмотреть в документации: https://gym.openai.com/envs/Taxi-v3/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ud0_kfOGLUwo"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwqXBprJKk26",
    "outputId": "66d5aeeb-b0ba-436c-e404-3c6686700fb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Phe3a5aTQBsd"
   },
   "outputs": [],
   "source": [
    "def show_progress(rewards_batch, log, reward_range=None):\n",
    "    \"\"\"\n",
    "    Удобная функция, которая отображает прогресс обучения.\n",
    "    \"\"\"\n",
    "\n",
    "    if reward_range is None:\n",
    "        reward_range = [-990, +10]\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    log.append([mean_reward])\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_Pbvpm4Kk2-"
   },
   "source": [
    "#### Задание 1\n",
    "\n",
    "Создайте таблицу из нулей, используя информацию из окружения о количестве состояний и действий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_bddYTsDQmVL"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# гиперпараметры алгоритма\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "episodes_number = 10001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8QjwLO_CKk3B"
   },
   "outputs": [],
   "source": [
    "def initialize_q_table(observation_space, action_space):\n",
    "    # подсказка смотрим env.observation_space.n и env.action_space.n\n",
    "    # q_table_ = [state][action]\n",
    "    ####### Здесь ваш код ########\n",
    "    q_table_ = np.zeros((observation_space, action_space))\n",
    "    ##############################\n",
    "    return q_table_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa8ZqT0NepxL",
    "outputId": "5cea211f-fed2-47cb-a932-cc185a056d1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(initialize_q_table(env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pG9YEmftQtu0"
   },
   "source": [
    "#### Задание 2\n",
    "\n",
    "Допишите недостающий код для алгоритма Q-обучения\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение.\n",
    "\n",
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686\">\n",
    "\n",
    "\n",
    "Раскроем скобки:\n",
    "$$Q(s,a)\\leftarrow (1 - \\alpha) \\times Q(s,a)+\\alpha \\times \\big (r(s)+\\gamma\\max_{a'}Q(a',s')\\big ).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "DUthRLiaIuuV",
    "outputId": "a35b741e-01eb-4201-c87e-498144160206"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000, Reward: 7\n"
     ]
    }
   ],
   "source": [
    "# определяем память, в которой будет храниться Q(s,a)\n",
    "q_table = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
    "log = []\n",
    "rewards_batch = []\n",
    "\n",
    "for i in range(1, episodes_number):\n",
    "    state = env.reset()\n",
    "\n",
    "    episode, r, episode_reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # получаем жадное действие\n",
    "        action_greed = np.argmax(q_table[state])\n",
    "        # выбираем действие, используя eps-greedy исследование среды\n",
    "        # с вероятностью epsilon выбираем случайное действие, иначе \n",
    "        # используем жадно действие \n",
    "        # подсказка: для выбора случайного действия можно использовать \n",
    "        # методы среды - env.action_space.sample()\n",
    "        # action = \n",
    "        ####### Здесь ваш код ########\n",
    "        random_choice = np.random.uniform()\n",
    "        if random_choice<=epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = action_greed\n",
    "        ##############################\n",
    "        \n",
    "        # выполняем действие в среде \n",
    "        next_state, r, done, info = env.step(action) \n",
    "        \n",
    "        # получаем old_value и value_estimate (estimate of optimal future value)\n",
    "        old_value = q_table[state, action]\n",
    "        next_value_estimate = np.max(q_table[next_state])\n",
    "        \n",
    "        # посчитайте new_value (temporal difference target)\n",
    "        # new_value = \n",
    "        ####### Здесь ваш код ########\n",
    "        new_value = r + gamma*next_value_estimate\n",
    "        ##############################\n",
    "        \n",
    "        q_table[state, action] = (1 - alpha) * old_value + alpha * new_value\n",
    "\n",
    "        state = next_state\n",
    "        episode += 1\n",
    "        episode_reward += r\n",
    "    rewards_batch.append(episode_reward)\n",
    "     \n",
    "    if i % 100 == 0:\n",
    "        show_progress(rewards_batch, log)\n",
    "        rewards_batch = []\n",
    "        print(f\"Episode: {i}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ],\n",
       "       [-0.90686814, -0.90523563, -0.90702137, -0.9047017 , -0.90083621,\n",
       "        -5.27892124],\n",
       "       [-0.86777394, -0.81857562, -0.86778666, -0.81951819, -0.7077875 ,\n",
       "        -5.26772776],\n",
       "       ...,\n",
       "       [-0.64807418, -0.46553961, -0.62562073, -0.65578903, -1.        ,\n",
       "        -1.85426527],\n",
       "       [-0.8896607 , -0.88873138, -0.89284294, -0.88838404, -2.58246865,\n",
       "        -1.87818635],\n",
       "       [ 0.10874138, -0.0905975 , -0.189     ,  3.99958315, -1.44208256,\n",
       "        -1.51398752]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCvfdLORQOf0"
   },
   "source": [
    "### Интерпретация результатов:\n",
    "Если все сделано правильно, то график должен выйти на плато около 0. Значение вознаграждение будет в диапазоне [-5, 10], за счет случайного выбора начальной позиции такси и пассажира. Попробуйте изменить гиперпараметры и сравните результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7sfWBeBq8Wx"
   },
   "source": [
    "## 4. Аппроксимация Q-функции\n",
    "\n",
    "В данном пункте мы будем использовать библиотеку pytorch для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "mXq0LL5sxmQ4"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "13uA4vJMxm0o"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# plt.imshow(env.render(\"rgb_array\"))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJNuUIXUxo3F"
   },
   "source": [
    "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "<img src=\"https://raw.githubusercontent.com/Tviskaron/mipt/master/2020/RL/figures/DQN.svg\">\n",
    "Для начала попробуйте использовать только полносвязные слои (``torch.nn.Linear``) и простые активационные функции (``torch.nn.ReLU``). Сигмоиды и другие функции не будут работать с ненормализованными входными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "scnIW4CKxp-8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CS9rWHk4xq6T"
   },
   "source": [
    "Определяем граф вычислений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "5qM-DR3uxr44"
   },
   "outputs": [],
   "source": [
    "# network = nn.Sequential(\n",
    "# torch.nn.Linear(state_dim[0], ...),\n",
    "# ...\n",
    "####### Здесь ваш код ########\n",
    "network = nn.Sequential(\n",
    "    torch.nn.Linear(state_dim[0], n_actions),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(n_actions, 5),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(5, 5),\n",
    "    nn.ReLU(),\n",
    "    torch.nn.Linear(5, n_actions),\n",
    "    \n",
    "\n",
    ")\n",
    "##############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "VVHBpe87xtmS"
   },
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    сэмплируем (eps greedy) действие\n",
    "    \"\"\"\n",
    "    state = torch.tensor(state[None], dtype=torch.float32)\n",
    "    q_values = network(state).detach().numpy()\n",
    "    \n",
    "    # action = \n",
    "    ####### Здесь ваш код ########\n",
    "    random_choice = np.random.uniform()\n",
    "    if random_choice<=epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_values)\n",
    "    ##############################\n",
    "    return int(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYDZqXvaxuw8",
    "outputId": "4d0d020b-ca18-43d7-890e-aa92d116459a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e=0.0 тесты пройдены\n",
      "e=0.1 тесты пройдены\n",
      "e=0.5 тесты пройдены\n",
      "e=1.0 тесты пройдены\n"
     ]
    }
   ],
   "source": [
    "s = env.reset()\n",
    "assert tuple(network(torch.tensor([s]*3, dtype=torch.float32)).size()) == (\n",
    "    3, n_actions), \"Убедитесь, что модель переводит s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert isinstance(list(network.modules(\n",
    "))[-1], nn.Linear), \"убедитесь, что вы предсказываете q без нелинейности\"\n",
    "assert isinstance(get_action(\n",
    "    s), int), \"убедитесь, что функция get_action() возвращает только одно действие типа integer\" % (type(get_action(s)))\n",
    "\n",
    "# проверяем исследование среды\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount(\n",
    "        [get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] -\n",
    "               10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] -\n",
    "                       10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f тесты пройдены' % eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQQ393Vjxwa6"
   },
   "source": [
    "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2,$$\n",
    "где\n",
    "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние \n",
    "* $\\gamma$ дисконтирующий множетель.\n",
    "\n",
    "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Эта та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q0ONl7rHxvZw"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False):\n",
    "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
    "    \n",
    "    # переводим входные данные в тензоры\n",
    "    states = torch.tensor(states, dtype=torch.float32)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size] \n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
    "    \n",
    "    \n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32) # shape: [batch_size, state_size]\n",
    "    is_done = torch.tensor(is_done, dtype=torch.uint8)    # shape: [batch_size]\n",
    "\n",
    "    # получаем значения q для всех действий из текущих состояний\n",
    "    predicted_qvalues = network(states)\n",
    "\n",
    "    # получаем q-values для выбранных действий\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
    "\n",
    "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
    "    # predicted_next_qvalues =\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
    "    # next_state_values =\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # вычисляем target q-values для функции потерь, \n",
    "    # что соответствует выражению в квадртаных скобках\n",
    "    #  target_qvalues_for_actions =\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    # для последнего действия используем \n",
    "    # упрощенную формулу Q(s,a) = r(s,a), \n",
    "    # т.к. s' для него не существует\n",
    "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    # MSE loss для минимизации\n",
    "    loss = torch.mean((predicted_qvalues_for_actions - target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZtPqvVwxzCu"
   },
   "outputs": [],
   "source": [
    "# небольшие проверки\n",
    "s = env.reset()\n",
    "a = env.action_space.sample()\n",
    "next_s, r, done, _ = env.step(a)\n",
    "loss = compute_td_loss([s], [a], [r], [next_s], [done], check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert len(loss.size()) == 0, \"функция должна вычислять скалярный loss - среднее по батчу\"\n",
    "assert np.any(next(network.parameters()).grad.detach().numpy() !=\n",
    "              0), \"loss должен быть дифференцируемым по весам сети\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lk5wfnQdx0GZ"
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(network.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KC8axVwLx0tD"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)\n",
    "        next_s, r, done, _ = env.step(a)\n",
    "\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "            compute_td_loss([s], [a], [r], [next_s], [done]).backward()\n",
    "            opt.step()\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oppnr-ux2Er"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-y8pRD4x2ek"
   },
   "outputs": [],
   "source": [
    "for i in range(150):\n",
    "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "\n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"убедитесь, что epsilon не становится < 0\"\n",
    "\n",
    "    if np.mean(session_rewards) > 200:\n",
    "        print(\"Принято!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Nt9sDgSx4SK"
   },
   "source": [
    "### Интерпретация результатов:\n",
    "\n",
    "Добро пожаловать в мир глубокого обучения с подкреплением! Не ждите, что вознаграждение агента будет увеличиваться плавно. Может быть оно начнет расти, если сочтет ваш код достойным :)\n",
    "\n",
    "А если серьезно,\n",
    "* __mean reward__ $-$ это среднее вознаграждение за игру. При правильной реализации оно может оставаться низким в течение первых 10 эпох, а затем начнет расти, и сойдется к ~50-100 эпохе, в зависимости от архитектуры сети.\n",
    "* Если со сходимость возникли проблемы $-$ попробуйте увеличить количество скрытых нейронов или обратите внимание на эпсилон.\n",
    "* __epsilon__ $-$ agent's willingness to explore. Если вы видите, что эпислон находится на уровне <0.01, до того, как агент достиг вознаграждения >= 200, установите первоначальное значение 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i6m2LRUBCmFf"
   },
   "source": [
    "### Посмотрим на результаты:\n",
    "\n",
    "Подключаем визуализацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8ile4WJCmYU"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !wget https://gist.githubusercontent.com/Tviskaron/4d35eabce2e057dd2ea49a00b00aaa41/raw/f1e25fc6ac6d8f11cb585559ce8b2ab9ffefd67b/colab_render.sh -O colab_render.sh -q\n",
    "    !sh colab_render.sh\n",
    "    !wget https://gist.githubusercontent.com/Tviskaron/d91decc1ca5f1b09af2f9f080011a925/raw/0d3474f65b4aea533996ee00edf99a37e4da5561/colab_render.py -O colab_render.py -q \n",
    "    import colab_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-s3nmP5CoGa"
   },
   "outputs": [],
   "source": [
    "# библиотеки и функции, которые потребуеются для показа видео\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "from gym.envs.classic_control import rendering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "org_constructor = rendering.Viewer.__init__\n",
    "\n",
    "\n",
    "def constructor(self, *args, **kwargs):\n",
    "    org_constructor(self, *args, **kwargs)\n",
    "    self.window.set_visible(visible=False)\n",
    "\n",
    "\n",
    "rendering.Viewer.__init__ = constructor\n",
    "\n",
    "\n",
    "def show_video(folder=\"./video\"):\n",
    "    mp4list = glob.glob(folder + '/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5bxrCYsCprE"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env, \"./video\", force=True)\n",
    "\n",
    "generate_session(env, epsilon=0, train=False)\n",
    "\n",
    "env.close()\n",
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Seminar-02-task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
