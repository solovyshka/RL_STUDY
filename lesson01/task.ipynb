{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkG_ZcvCuQJ0"
   },
   "source": [
    "# Машинное обучение с подкреплением. Сбербанк весна 2021.\n",
    "## Семинар 1: OpenAI gym, CrossEntropy Method (CEM), Deep CEM.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh44RkMOlwmr"
   },
   "source": [
    "Импортируем необходимые библиотеки и настраиваем визуализацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "G3ImfRnglwmr"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import colab\n",
    "    COLAB = True\n",
    "except ModuleNotFoundError:\n",
    "    COLAB = False\n",
    "    pass\n",
    "\n",
    "if COLAB:\n",
    "    !wget https://gist.githubusercontent.com/Tviskaron/4d35eabce2e057dd2ea49a00b00aaa41/raw/f1e25fc6ac6d8f11cb585559ce8b2ab9ffefd67b/colab_render.sh -O colab_render.sh -q\n",
    "    !sh colab_render.sh\n",
    "    !wget https://gist.githubusercontent.com/Tviskaron/d91decc1ca5f1b09af2f9f080011a925/raw/0d3474f65b4aea533996ee00edf99a37e4da5561/colab_render.py -O colab_render.py -q \n",
    "    import colab_render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.18.0.tar.gz (1.6 MB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.6.1-cp38-cp38-win_amd64.whl (32.7 MB)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\vvsol\\anaconda3\\envs\\rl_env\\lib\\site-packages (from gym) (1.20.1)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0\n",
      "  Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Collecting Pillow<=7.2.0\n",
      "  Downloading Pillow-7.2.0-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0\n",
      "  Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "Building wheels for collected packages: gym, future\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.18.0-py3-none-any.whl size=1656454 sha256=c7d6d69bfef2f6d6cf3a3ef3f600d5f57c205580ad6cf713f90c3c515f31c19a\n",
      "  Stored in directory: c:\\users\\vvsol\\appdata\\local\\pip\\cache\\wheels\\d8\\e7\\68\\a3f0f1b5831c9321d7523f6fd4e0d3f83f2705a1cbd5daaa79\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491062 sha256=4e5fe993896ffc1337b836e487d9fd5261476b6d284645b51e840f345a2fec0b\n",
      "  Stored in directory: c:\\users\\vvsol\\appdata\\local\\pip\\cache\\wheels\\8e\\70\\28\\3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built gym future\n",
      "Installing collected packages: scipy, future, pyglet, Pillow, cloudpickle, gym\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.1.2\n",
      "    Uninstalling Pillow-8.1.2:\n",
      "      Successfully uninstalled Pillow-8.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an EnvironmentError: [WinError 5] Отказано в доступе: 'C:\\\\Users\\\\vvsol\\\\Anaconda3\\\\envs\\\\RL_env\\\\Lib\\\\site-packages\\\\~il\\\\_imaging.cp38-win_amd64.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YjP0JuoBlwmr"
   },
   "outputs": [],
   "source": [
    "# библиотеки и функции, которые потребуеются для показа видео\n",
    "\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "from gym.envs.classic_control import rendering\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "org_constructor = rendering.Viewer.__init__\n",
    "\n",
    "\n",
    "def constructor(self, *args, **kwargs):\n",
    "    org_constructor(self, *args, **kwargs)\n",
    "    self.window.set_visible(visible=False)\n",
    "\n",
    "\n",
    "rendering.Viewer.__init__ = constructor\n",
    "\n",
    "\n",
    "def show_video(folder=\"./video\"):\n",
    "    mp4list = glob.glob(folder + '/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = sorted(mp4list, key=lambda x: x[-15:], reverse=True)[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else:\n",
    "        print(\"Could not find video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBu75eDUlwmr"
   },
   "source": [
    "## 1. OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVunU7AGlwmr"
   },
   "source": [
    "[Gym](https://gym.openai.com) $-$ это набор инструментов для разработки и сравнения алгоритмов обучения с подкреплением, который также включает в себя большой [набор окружений](https://gym.openai.com/envs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B44xeWtLlwmr"
   },
   "source": [
    "### Создание окружения\n",
    "\n",
    "Для создания окружения используется функция ```gym.make(<имя окружения>)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pJP5uWS6lwmr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [-0.5052163  0.       ]\n",
      "next_state: [-0.5063541 -0.0011378] , r: -1.0, done: False, info: {}\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Создаем окружение\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Инициализируем окружение\n",
    "state = env.reset()\n",
    "print(f\"state: {state}\")\n",
    "\n",
    "# Выполняем действие в среде \n",
    "next_state, r, done, info = env.step(0)\n",
    "print(f\"next_state: {next_state} , r: {r}, done: {done}, info: {info}\")\n",
    "\n",
    "# Закрываем окружение\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1qnheA_lwmr"
   },
   "source": [
    "### Основные методы окружения:\n",
    "\n",
    "* ``reset()`` $-$ инициализация окружения, возвращает первое наблюдение (состояние).  \n",
    "* ``step(a)`` $-$ выполнить в среде действие $\\mathbf{a}$ и получить кортеж: $\\mathbf{\\langle s_{t+1}, r_t, done, info \\rangle}$, где $\\mathbf{s_{t+1}}$ - следующее состояние, $\\mathbf{r_t}$ - вознаграждение, $\\mathbf{done}$ - флаг заверешния, $\\mathbf{info}$ - дополнительная информация\n",
    "\n",
    "### Дополнительные методы:\n",
    "* ``render()`` $-$ визуализация текущего состояния среды (удобно, если мы запускаем локально, в колабе ничего не увидим)\n",
    "\n",
    "* ``close()`` $-$ закрывает окружение "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4X0Nr2llwmr"
   },
   "source": [
    "### Свойства среды:\n",
    "* ``env.observation_space`` $-$ информация о пространстве состояний\n",
    "* ``env.action_space`` $-$ информация о пространстве действий\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ilcxwQOblwmr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.observation_space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "env.action_space: Discrete(3)\n"
     ]
    }
   ],
   "source": [
    "print(f\"env.observation_space: {env.observation_space}\")\n",
    "print(f\"env.action_space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geFkzXSUlwmr"
   },
   "source": [
    "### Среда ``MountainCar-v0``\n",
    "\n",
    "Информацию о любой среде можно найти в [исходниках](https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py) или на [сайте](https://gym.openai.com/envs/MountainCar-v0/). О ``MountainCar-v0`` мы можем узнать следующее: \n",
    "\n",
    "#### Задание:\n",
    "Автомобиль едет по одномерному треку между двумя холмами. Цель состоит в том, чтобы заехать на правый холм; однако двигатель машины недостаточно мощный, чтобы взобраться на холм за один проход. Следовательно, единственный способ добиться успеха $-$ это двигаться вперед и назад, чтобы набрать обороты.\n",
    "\n",
    "#### Пространство состояний Box(2):\n",
    "\n",
    "\n",
    "\n",
    "Num | Observation  | Min  | Max  \n",
    "----|--------------|------|----   \n",
    "0   | position     | -1.2 | 0.6\n",
    "1   | velocity     | -0.07| 0.07\n",
    "\n",
    "\n",
    "#### Пространство действий Discrete(3):\n",
    "\n",
    "\n",
    "\n",
    "Num | Action|\n",
    "----|-------------|\n",
    "0   | push left   |\n",
    "1   | no push     |\n",
    "2   | push right  |\n",
    "\n",
    "* Вознаграждения: -1 за каждый шаг, пока не достигнута цель \n",
    "\n",
    "* Начальное состояние: Случайная позиция от -0.6 до -0.4 с нулевой скоростью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzLOupOilwmr"
   },
   "source": [
    "### Пример со случайной стратегией:\n",
    "\n",
    "Для выбора действия используется ``env.action_space.sample()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UT40wJ-8lwmr"
   },
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b9b86e1c66a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# проводим инициализацию и запоминаем начальное состояние\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_env\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_env\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_env\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         )\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_frames_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\RL_env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "# создаем окружение\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "# добавляем wrapper (обертку), чтобы задать ограничение на число шагов в среде\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=250)\n",
    "# добавляем визуализацию\n",
    "env = gym.wrappers.Monitor(env, \"./video\", force=True)\n",
    "\n",
    "# проводим инициализацию и запоминаем начальное состояние\n",
    "s = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # выполняем действие, получаем s, r, done, info\n",
    "    s, r, done, _ = env.step(env.action_space.sample())\n",
    "    \n",
    "env.close()\n",
    "\n",
    "# Сначала закрываем окружение, чтобы видео записалось полностью\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0E68j0A5lwmr"
   },
   "source": [
    "### Задание 1:\n",
    "В среде MountainCar-v0 мы хотим, чтобы машина достигла флага. Давайте решим эту задачу, не используя обучение с подкреплением. Модифицируйте код функции act() ниже для выполнения этого задания. Функция получает на вход состояние среды и должна вернуть действие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65_fcmjulwmr"
   },
   "outputs": [],
   "source": [
    "def act(s):\n",
    "    # список возможных действий\n",
    "    left, stop, right = 0, 1, 2\n",
    "    \n",
    "    # позиция и скорость\n",
    "    position, velocity = s\n",
    "    # пример: можем попробовать ехать только влево\n",
    "    # action = actions['left']\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mw1YyFB2lwmr"
   },
   "source": [
    "Проверяем решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39BSkqsqlwmr"
   },
   "outputs": [],
   "source": [
    "env = gym.wrappers.TimeLimit(gym.make(\"MountainCar-v0\"), max_episode_steps=250)\n",
    "env = gym.wrappers.Monitor(env, \"./video\", force=True)\n",
    "\n",
    "# проводим инициализацию и запоминаем начальное состояние\n",
    "s = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    # выполняем действие, получаем s, r, done, info\n",
    "    s, r, done, _ = env.step(act(s))\n",
    "    \n",
    "if s[0] > 0.47:\n",
    "    print(\"Принято!\")\n",
    "else:\n",
    "    print(\"\"\"Исправьте функцию выбора действия!\"\"\")\n",
    "\n",
    "\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM7vcYORuQJ1"
   },
   "source": [
    "## 2. Crossentropy Method\n",
    "\n",
    "В этой пункте мы посмотрим на то, как решить задачи RL с помощью метода crossentropy.\n",
    "\n",
    "Рассмотрим пример с задачей Taxi [Dietterich, 2000]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5yU3RZuuQJ2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xyi1DDjQuQJ8"
   },
   "outputs": [],
   "source": [
    "n_states  = env.observation_space.n\n",
    "n_actions = env.action_space.n  \n",
    "\n",
    "print(f\"состояний: {n_states} действий: {n_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR-L-rHquQKA"
   },
   "source": [
    "В этот раз нашей стратегией будет вероятностной распределение. \n",
    "\n",
    "$\\pi(s,a) = P(a|s)$\n",
    "\n",
    "Для задачи такси мы можем использовать таблицу: \n",
    "\n",
    "policy[s,a] = P(выбрать действие a | в состоянии s)\n",
    "\n",
    "Создадим \"равномерную\" стратегию в виде двумерного массива с \n",
    "равномерным распределением по действиям и сгенерируем игровую сессию с такой стратегией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVBlXo-huQKB"
   },
   "outputs": [],
   "source": [
    "def initialize_policy(n_states, n_actions):\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    return policy\n",
    "\n",
    "policy = initialize_policy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xqVJHT4uQKE"
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDx_ek7vuQKH"
   },
   "source": [
    "### Генерация сессий взаимодейтсвия со средой.\n",
    "\n",
    "Мы будем запоминать все состояния, действия и вознаграждения за эпизод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlsAeF7EuQKI"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, policy, t_max=10**4):\n",
    "    \"\"\"\n",
    "    Игра идет до конца эпизода или до t_max шагов в окружении. \n",
    "    :param policy: [n_states,n_actions] \n",
    "    :returns: states - список состояний, actions - список действий, total_reward - итоговое вознаграждение\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # Подсказка: вы можете использовать np.random.choice для сэмплирования\n",
    "        # https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html\n",
    "        # a = \n",
    "        ####### Здесь ваш код ########\n",
    "        raise NotImplementedError\n",
    "        ##############################\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Record information we just got from the environment.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM2-QI3UuQKL"
   },
   "outputs": [],
   "source": [
    "s, a, r = generate_session(env, policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWqyUSLJuQKP"
   },
   "outputs": [],
   "source": [
    "# посмотрим на изначальное распределение вознаграждения\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(env, policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUDOpKxHuQKS"
   },
   "source": [
    "### Реализация метода crossentropy  \n",
    "\n",
    "Наша задача - выделить лучшие действия и состояния, т.е. такие, при которых было лучшее вознаграждение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGIweAi5uQKT"
   },
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, \n",
    "                  rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Выбирает состояния и действия с заданным перцентилем (rewards >= percentile)\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "    \n",
    "    :returns: elite_states, elite_actions - одномерные \n",
    "    списки состояния и действия, выбранных сессий\n",
    "    \"\"\"\n",
    "    # нужно найти порог вознаграждения по процентилю\n",
    "    # reward_threshold =\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    \n",
    "    # в соответствии с найденным порогом - отобрать \n",
    "    # подходящие состояния и действия\n",
    "    # elite_states = \n",
    "    # elite_actions = \n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q34NZBHHuQKW"
   },
   "outputs": [],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_30 = select_elites(states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(\n",
    "    test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1]) \\\n",
    "       and np.all(\n",
    "    test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]), \\\n",
    "    \"Для процентиля 0 необходимо выбрать все состояния \" \\\n",
    "    \"и действия в хронологическом порядке\"\n",
    "\n",
    "assert np.all(test_result_30[0] == [4, 2, 0, 2, 3, 1])\\\n",
    "   and np.all(test_result_30[1] == [3, 2, 0, 1, 3, 3]), \\\n",
    "    \"Для процентиля 30 необходимо выбрать \" \\\n",
    "    \"состояния/действия из [3:]\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "       np.all(test_result_90[1] == [3, 3]), \\\n",
    "    \"Для процентиля 90 необходимо выбрать состояния \" \\\n",
    "    \"действия одной игры\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and \\\n",
    "       np.all(test_result_100[1] == [3, 3]), \\\n",
    "    \"Проверьте использование знаков: >=,  >. \" \\\n",
    "    \"Также проверьте расчет процентиля\"\n",
    "print(\"Тесты пройдены!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZzLzP7PuQKb"
   },
   "source": [
    "Теперь мы хотим написать обновляющуюся стратегию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PecfT_xEuQKc"
   },
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Новой стратегией будет:\n",
    "    policy[s_i,a_i] ~ #[вхождения  si/ai в лучшие states/actions]\n",
    "    \n",
    "    Не забудьте про нормализацию состояний.\n",
    "    Если какое-то состояние не было посещено, \n",
    "    то используйте равномерное распределение 1./n_actions\n",
    "    \n",
    "    :param elite_states:  список состояний\n",
    "    :param elite_actions: список действий\n",
    "    \"\"\"\n",
    "    new_policy = np.zeros([n_states,n_actions])\n",
    "    # обновляем стратегию - нормируем новые частоты \n",
    "    # действий и не забываем про непосещенные состояния\n",
    "    # new_policy[state, a]         \n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsCFXHMRuQKf"
   },
   "outputs": [],
   "source": [
    "elite_states, elite_actions = (\n",
    "    [1, 2, 3, 4, 2, 0, 2, 3, 1],\n",
    "    [0, 2, 4, 3, 2, 0, 1, 3, 3])\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(\n",
    "    new_policy).all(), \"Стратегия не должна содержать \" \\\n",
    "                       \"NaNs или +-inf. Проверьте \" \\\n",
    "                       \"деление на ноль. \"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Стратегия не должна содержать \" \\\n",
    "                      \"отрицательных вероятностей \"\n",
    "assert np.allclose(new_policy.sum(axis=-1),\n",
    "                   1), \"Суммарная\\ вероятность действий\"\\\n",
    "                       \"для состояния должна равняться 1\"\n",
    "reference_answer = np.array([\n",
    "    [1., 0., 0., 0., 0.],\n",
    "    [0.5, 0., 0., 0.5, 0.],\n",
    "    [0., 0.33333333, 0.66666667, 0., 0.],\n",
    "    [0., 0., 0., 0.5, 0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Тесты пройдены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYRLBwtvuQKi"
   },
   "source": [
    "### Цикл обучения\n",
    "\n",
    "Визуализириуем наш процесс обучения и также будем измерять распределение получаемых за сессию вознаграждений "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tmBnonZCuQKj"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +10]):\n",
    "    \"\"\"\n",
    "    Удобная функция, для визуализации результатов.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "    \n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GXhGLvjPuQKm"
   },
   "outputs": [],
   "source": [
    "# инициализируем стратегию\n",
    "policy = initialize_policy(n_states, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYncUOBfuQKo"
   },
   "outputs": [],
   "source": [
    "n_sessions = 250  # количество сессий для сэмплирования\n",
    "percentile = 50  # перцентиль \n",
    "learning_rate = 0.5 # то как быстро стратегия будет обновляться \n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    # генерируем n_sessions сессий\n",
    "    # time sessions = []\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    states_batch,actions_batch,rewards_batch = zip(*sessions)\n",
    "    # отбираем лучшие действия и состояния ###\n",
    "    # elite_states, elite_actions = \n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    # обновляем стратегию\n",
    "    # new_policy =\n",
    "    ####### Здесь ваш код ########\n",
    "    raise NotImplementedError\n",
    "    ##############################\n",
    "    \n",
    "    policy = learning_rate * new_policy + (1 - learning_rate) * policy\n",
    "\n",
    "    # выводим график с результатами\n",
    "    show_progress(rewards_batch, log, percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xIsESNpuQKr"
   },
   "source": [
    "### Посмотрим на результаты\n",
    "Задача такси быстро сходится, начиная с вознаграждения -1000 к почти оптимальному значению, а потом опять падает до -50/-100. Это вызвано случайностью в самом окружении $-$ случайное начальное состояние пассажира и такси, в начале каждого эпизода. \n",
    "\n",
    "В случае если алгоритм CEM не сможет научиться тому, как решить задачу из какого-то стартового положения, он просто отбросит этот эпизод, т.к. не будет сессий, которые переведут этот эпизод в топ лучших. \n",
    "\n",
    "Для решения этой проблемы можно уменьшить threshold (порог лучших состояний) или изменить способ оценки стратегии, используя новую стратегию, полученную из каждого начального состояния и действия (теоретически правильный способ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-nMGpEfuQKs"
   },
   "source": [
    "## 3. Deep CEM\n",
    "\n",
    "В данной части мы рассмотрим применение CEM вместе с нейронной сетью.\n",
    "Будем обучать многослойную нейронную сеть для решения простой задачи с непрерывным пространством действий.\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/8f39c7f54a7798e7f80c9ec5c0bb610696e5c5b7/68747470733a2f2f7469702e64756b652e6564752f696e646570656e64656e745f6c6561726e696e672f677265656b2f6c6573736f6e2f64696767696e675f6465657065725f66696e616c2e6a7067\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWFuA1hWuQKs"
   },
   "source": [
    "Будем тестировать нашего нового агента на известной задаче перевернутого маятника с непрерывным пространством состояний.\n",
    "https://gym.openai.com/envs/CartPole-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZhZahUGuQKt"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "                                        \n",
    "print(f\"состояний: {state_dim} действий: {n_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgRpAtU-uQKw"
   },
   "source": [
    "### Стратегия с нейронной сетью\n",
    "\n",
    "Попробуем заменить метод обновления вероятностей на нейронную сеть. \n",
    "Будем пользоваться упрощенной реализацией нейронной сети из пакета Scikit-learn.\n",
    "Нам потребуется: \n",
    "* agent.partial_fit(states, actions) - делает один проход обучения по данным. Максимизирует вероятность :actions: из :states:\n",
    "* agent.predict_proba(states) - предсказыает вероятность каждого из действий, в виде матрицы размера [len(states), n_actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fOPMKcYuQKx"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "agent = MLPClassifier(\n",
    "    hidden_layer_sizes=(20, 20),\n",
    "    activation='tanh',\n",
    ")\n",
    "\n",
    "# инициализируем агента под заданное пространство состояний и действий\n",
    "agent.partial_fit([env.reset()] * n_actions, range(n_actions), range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R9W-5nV3uQK0"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, agent, t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # предсказываем вероятности действий по сети и \n",
    "        # выбираем одно действие\n",
    "        # probs = \n",
    "        # a = \n",
    "        ####### Здесь ваш код ########\n",
    "        raise NotImplementedError\n",
    "        ##############################\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pntOSnguQK2"
   },
   "outputs": [],
   "source": [
    "dummy_states, dummy_actions, dummy_reward = generate_session(env, agent, t_max=5)\n",
    "print(\"состояния:\", np.stack(dummy_states))\n",
    "print(\"действия:\", dummy_actions)\n",
    "print(\"вознаграждение:\", dummy_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVDwmDFAuQK4"
   },
   "outputs": [],
   "source": [
    "n_sessions = 200\n",
    "percentile = 50\n",
    "log = []\n",
    "\n",
    "for i in range(100):\n",
    "    sessions = [generate_session(env, agent) for _ in range(n_sessions)]\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, zip(*sessions))\n",
    "    \n",
    "    elite_states, elite_actions = select_elites(states_batch,actions_batch,rewards_batch, percentile)\n",
    "    \n",
    "    # обновляем стратегию, для предсказания лучших состояний\n",
    "    # elite_actions(y) из elite_states(X)\n",
    "    agent.fit(elite_states, elite_actions)\n",
    "    \n",
    "    show_progress(rewards_batch, log, percentile, reward_range=[0, np.max(rewards_batch)])\n",
    "\n",
    "    if np.mean(rewards_batch) > 190:\n",
    "        print(\"Принято!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6rDniBvlwms"
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ESsETPklwms",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = gym.wrappers.Monitor(env, \"./video\", force=True)\n",
    "\n",
    "generate_session(env, agent)\n",
    "\n",
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxjN0BR0lwms"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "RL_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
